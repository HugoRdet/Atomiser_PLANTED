{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e28fc5-b768-412c-8e0a-91aece41507a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10bac0d70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training.perceiver import*\n",
    "from training.utils import*\n",
    "from training.losses import*\n",
    "from training.VIT import*\n",
    "from training.ResNet import*\n",
    "from collections import defaultdict\n",
    "from training import*\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "import einops as einops\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Reduce\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4cfe7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ids=get_ids_in_folder(base_path=\"./data/public/1.0.1/test/\")\n",
    "\n",
    "idx=10\n",
    "data=load_npz_by_id(idx,base_path=\"./data/public/1.0.1/test/\")\n",
    "\n",
    "\n",
    "\n",
    "#data=np.load(\"path\")\n",
    "tmp_name=data[\"alos\"][idx] #shape : 5000;4;4;4;3 (batch / time / h / w / c)\n",
    "print(tmp_name.shape)\n",
    "\n",
    "#3456\n",
    "#11520\n",
    "#1920\n",
    "#420\n",
    "#192\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c312fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples to write: 135961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 28/28 [00:57<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "#create_dataset(name=\"tiny\", mode=\"test\",max_imgs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca540407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f06a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=read_yaml(\"./training/configs/config_test-Atomiser_Atos.yaml\")\n",
    "transconfig=transformations_config_flair(\"./data/bands_info/bands.yaml\",config)\n",
    "\n",
    "dataset=CustomPlanted(\"./data/custom_planted/tiny_train.h5\", config=config, trans_config=transconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decd240e",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'tmp_infos' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PLANTED/training/utils/utils_dataset_h5.py:451\u001b[39m, in \u001b[36mCustomPlanted.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    449\u001b[39m s1_dates = f[\u001b[33m\"\u001b[39m\u001b[33ms1_timestamps\u001b[39m\u001b[33m\"\u001b[39m][idx]  \u001b[38;5;66;03m# Just example — you can build years/months/days here if needed\u001b[39;00m\n\u001b[32m    450\u001b[39m s1_mask = torch.tensor(f[\u001b[33m\"\u001b[39m\u001b[33ms1_mask\u001b[39m\u001b[33m\"\u001b[39m][idx], dtype=torch.float32)  \u001b[38;5;66;03m# e.g., first timestep, shape [12,12]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m tokens_s1=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrans_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_transformations_SAR\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms1_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms1_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m token_masks_wavelength.append(torch.ones(tokens_s1.shape[\u001b[32m0\u001b[39m])*\u001b[32m2\u001b[39m)\n\u001b[32m    453\u001b[39m tokens_list.append(tokens_s1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PLANTED/training/utils/utils_dataset_transformation.py:390\u001b[39m, in \u001b[36mtransformations_config_flair.apply_transformations_SAR\u001b[39m\u001b[34m(self, im_sen, dates_sen, mask_sen, mode)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(im_sen.shape[\u001b[32m1\u001b[39m]):       \n\u001b[32m    388\u001b[39m     time_encoding=\u001b[38;5;28mself\u001b[39m.time_processing(dates_sen[t],img_size)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m index_channel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtmp_infos\u001b[49m.keys())):\n\u001b[32m    392\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m index_channel==\u001b[32m2\u001b[39m:\n\u001b[32m    393\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'tmp_infos' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/hugoriffaud/Documents/PLANTED/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/Users/hugoriffaud/Documents/PLANTED/venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/hugoriffaud/Documents/PLANTED/checkpoints exists and is not empty.\n",
      "\n",
      "   | Name                      | Type                      | Params | Mode \n",
      "---------------------------------------------------------------------------------\n",
      "0  | metric_Acc_train          | MulticlassAccuracy        | 0      | train\n",
      "1  | metric_Acc_validation     | MulticlassAccuracy        | 0      | train\n",
      "2  | metric_Acc_test           | MulticlassAccuracy        | 0      | train\n",
      "3  | metric_F1_train           | MulticlassF1Score         | 0      | train\n",
      "4  | metric_F1_validation      | MulticlassF1Score         | 0      | train\n",
      "5  | metric_F1_test            | MulticlassF1Score         | 0      | train\n",
      "6  | metric_F1_train_freq      | MulticlassF1Score         | 0      | train\n",
      "7  | metric_F1_validation_freq | MulticlassF1Score         | 0      | train\n",
      "8  | metric_F1_test_freq       | MulticlassF1Score         | 0      | train\n",
      "9  | metric_F1_train_com       | MulticlassF1Score         | 0      | train\n",
      "10 | metric_F1_validation_com  | MulticlassF1Score         | 0      | train\n",
      "11 | metric_F1_test_com        | MulticlassF1Score         | 0      | train\n",
      "12 | metric_F1_train_rare      | MulticlassF1Score         | 0      | train\n",
      "13 | metric_F1_validation_rare | MulticlassF1Score         | 0      | train\n",
      "14 | metric_F1_test_rare       | MulticlassF1Score         | 0      | train\n",
      "15 | metric_IoU_val            | MulticlassJaccardIndex    | 0      | train\n",
      "16 | metric_IoU_test           | MulticlassJaccardIndex    | 0      | train\n",
      "17 | confmat_test              | MulticlassConfusionMatrix | 0      | train\n",
      "18 | encoder                   | Atomiser                  | 15.1 M | train\n",
      "19 | loss                      | CrossEntropyLoss          | 0      | train\n",
      "---------------------------------------------------------------------------------\n",
      "15.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.1 M    Total params\n",
      "60.550    Total estimated model params size (MB)\n",
      "112       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]Validation DataLoader created on rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugoriffaud/Documents/PLANTED/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugoriffaud/Documents/PLANTED/venv/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassJaccardIndex was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader created on rank: 0                                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugoriffaud/Documents/PLANTED/venv/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MulticlassF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/Users/hugoriffaud/Documents/PLANTED/venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 212/1075701 [00:18<25:35:39, 11.67it/s, v_num=16]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import torch\n",
    "import os\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "config_model = read_yaml(\"./training/configs/config_test-Atomiser_Atos.yaml\")\n",
    "labels = load_json_to_dict(\"./data/labels.json\")\n",
    "bands_yaml = \"./data/bands_info/bands.yaml\"\n",
    "\n",
    "trans_config = transformations_config_flair(bands_yaml, config_model)\n",
    "xp_name = \"test_Atos\"\n",
    "\n",
    "wand = False\n",
    "wandb_logger = None\n",
    "if wand:\n",
    "    if os.environ.get(\"LOCAL_RANK\", \"0\") == \"0\":\n",
    "        import wandb\n",
    "        wandb.init(\n",
    "            name=get_xp_name(config_model['encoder']) + \" modalities\",\n",
    "            project=\"Atos_modalities\",\n",
    "            config=config_model\n",
    "        )\n",
    "        wandb_logger = WandbLogger(project=\"tiny_modalities\")\n",
    "\n",
    "model = Model(config_model, wand=wand, name=xp_name, labels=labels)\n",
    "\n",
    "data_module = CustomPlantedDataModule(\n",
    "    \"./data/custom_planted/tiny\",\n",
    "    config=config_model,\n",
    "    trans_config=trans_config,\n",
    "    batch_size=config_model['dataset']['batchsize'],\n",
    ")\n",
    "\n",
    "data_module.setup()\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints/\",\n",
    "    filename=\"best_model-{epoch:02d}-{val_IoU:.4f}\",\n",
    "    monitor=\"val_IoU\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_IoU\",\n",
    "    min_delta=0.00,\n",
    "    patience=15,\n",
    "    verbose=False,\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    use_distributed_sampler=False,\n",
    "    #strategy=\"ddp\",\n",
    "    #devices=-1,\n",
    "    max_epochs=config_model[\"trainer\"][\"epochs\"],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=1,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    default_root_dir=\"./checkpoints/\"\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de252df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
